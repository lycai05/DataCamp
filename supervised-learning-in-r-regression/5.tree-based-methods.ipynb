{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a random forest model for bike rentals\n",
    "In this exercise you will again build a model to predict the number of bikes rented in an hour as a function of the weather, the type of day (holiday, working day, or weekend), and the time of day. You will train the model on data from the month of July.\n",
    "\n",
    "You will use the ranger package to fit the random forest model. For this exercise, the key arguments to the ranger() call are:\n",
    "\n",
    "- formula\n",
    "- data\n",
    "- num.trees: the number of trees in the forest.\n",
    "- respect.unordered.factors : Specifies how to treat unordered factor variables. We recommend setting this to \"order\" for regression.\n",
    "- seed: because this is a random algorithm, you will set the seed to get reproducible results\n",
    "\n",
    "Since there are a lot of input variables, for convenience we will specify the outcome and the inputs in the variables outcome and vars, and use paste() to assemble a string representing the model formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bikesJuly is in the workspace\n",
    "str(bikesJuly)\n",
    "# 'data.frame':\t744 obs. of  12 variables:\n",
    "#  $ hr        : Factor w/ 24 levels \"0\",\"1\",\"2\",\"3\",..: 1 2 3 4 5 6 7 8 9 10 ...\n",
    "#  $ holiday   : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n",
    "#  $ workingday: logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n",
    "#  $ weathersit: chr  \"Clear to partly cloudy\" \"Clear to partly cloudy\" \"Clear to partly cloudy\" \"Clear to partly cloudy\" ...\n",
    "#  $ temp      : num  0.76 0.74 0.72 0.72 0.7 0.68 0.7 0.74 0.78 0.82 ...\n",
    "#  $ atemp     : num  0.727 0.697 0.697 0.712 0.667 ...\n",
    "#  $ hum       : num  0.66 0.7 0.74 0.84 0.79 0.79 0.79 0.7 0.62 0.56 ...\n",
    "#  $ windspeed : num  0 0.1343 0.0896 0.1343 0.194 ...\n",
    "#  $ cnt       : int  149 93 90 33 4 10 27 50 142 219 ...\n",
    "#  $ instant   : int  13004 13005 13006 13007 13008 13009 13010 13011 13012 13013 ...\n",
    "#  $ mnth      : int  7 7 7 7 7 7 7 7 7 7 ...\n",
    "#  $ yr        : int  1 1 1 1 1 1 1 1 1 1 ...\n",
    "\n",
    "# Random seed to reproduce results\n",
    "seed\n",
    "# [1] 423563\n",
    "\n",
    "# the outcome column\n",
    "(outcome <- \"cnt\")\n",
    "# [1] \"cnt\"\n",
    "\n",
    "# The input variables\n",
    "(vars <- c(\"hr\", \"holiday\", \"workingday\", \"weathersit\", \"temp\", \"atemp\", \"hum\", \"windspeed\"))\n",
    "# [1] \"hr\"         \"holiday\"    \"workingday\" \"weathersit\" \"temp\"      \n",
    "# [6] \"atemp\"      \"hum\"        \"windspeed\"\n",
    "\n",
    "# Create the formula string for bikes rented as a function of the inputs\n",
    "(fmla <- paste(outcome, \"~\", paste(vars, collapse = \" + \")))\n",
    "#[1] \"cnt ~ hr + holiday + workingday + weathersit + temp + atemp + hum + windspeed\"\n",
    "\n",
    "# Load the package ranger\n",
    "library(ranger)\n",
    "\n",
    "# Fit and print the random forest model.\n",
    "(bike_model_rf <- ranger(fmla, \n",
    "                         bikesJuly, \n",
    "                         num.trees = 500, \n",
    "                         respect.unordered.factors = \"order\", \n",
    "                         seed = seed))\n",
    "# Ranger result\n",
    "\n",
    "# Call:\n",
    "#  ranger(fmla, bikesJuly, num.trees = 500, respect.unordered.factors = \"order\",      seed = seed) \n",
    "\n",
    "# Type:                             Regression \n",
    "# Number of trees:                  500 \n",
    "# Sample size:                      744 \n",
    "# Number of independent variables:  8 \n",
    "# Mtry:                             2 \n",
    "# Target node size:                 5 \n",
    "# Variable importance mode:         none \n",
    "# Splitrule:                        variance \n",
    "# OOB prediction error (MSE):       8230.568 \n",
    "# R squared (OOB):                  0.8205434"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict bike rentals with the random forest model\n",
    "In this exercise you will use the model that you fit in the previous exercise to predict bike rentals for the month of August.\n",
    "\n",
    "The predict() function for a ranger model produces a list. One of the elements of this list is predictions, a vector of predicted values. You can access predictions with the $ notation for accessing named elements of a list:\n",
    "\n",
    "```\n",
    "predict(model, data)$predictions\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the August data\n",
    "bikesAugust$pred <- predict(bike_model_rf, bikesAugust)$predictions\n",
    "\n",
    "# Calculate the RMSE of the predictions\n",
    "bikesAugust %>% \n",
    "  mutate(residual = cnt - pred)  %>% # calculate the residual\n",
    "  summarize(rmse  = sqrt(mean(residual^2)))      # calculate rmse\n",
    "#       rmse\n",
    "# 1 96.66032\n",
    "\n",
    "# Plot actual outcome vs predictions (predictions on x-axis)\n",
    "ggplot(bikesAugust, aes(x = pred, y = cnt)) + \n",
    "  geom_point() + \n",
    "  geom_abline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![predict_bike_rentals_with_the_random_forest_model](./figures/predict_bike_rentals_with_the_random_forest_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize random forest bike model predictions\n",
    "In the previous exercise, you saw that the random forest bike model did better on the August data than the quasiposson model, in terms of RMSE.\n",
    "\n",
    "In this exercise you will visualize the random forest model's August predictions as a function of time. The corresponding plot from the quasipoisson model that you built in a previous exercise is in the workspace for you to compare.\n",
    "\n",
    "Recall that the quasipoisson model mostly identified the pattern of slow and busy hours in the day, but it somewhat underestimated peak demands. You would like to see how the random forest model compares.\n",
    "\n",
    "The data frame bikesAugust (with predictions) is in the workspace. The plot quasipoisson_plot of quasipoisson model predictions as a function of time is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_two_weeks <- bikesAugust %>% \n",
    "  # Set start to 0, convert unit to days\n",
    "  mutate(instant = (instant - min(instant)) / 24) %>% \n",
    "  # Gather cnt and pred into a column named value with key valuetype\n",
    "  gather(key = valuetype, value = value, cnt, pred) %>%\n",
    "  # Filter for rows in the first two\n",
    "  filter(instant < 14) \n",
    "\n",
    "# Plot predictions and cnt by date/time \n",
    "ggplot(first_two_weeks, aes(x = instant, y = value, color = valuetype, linetype = valuetype)) + \n",
    "  geom_point() + \n",
    "  geom_line() + \n",
    "  scale_x_continuous(\"Day\", breaks = 0:14, labels = 0:14) + \n",
    "  scale_color_brewer(palette = \"Dark2\") + \n",
    "  ggtitle(\"Predicted August bike rentals, Random Forest plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visualize_random_forest_bike_model_predictions](./figures/visualize_random_forest_bike_model_predictions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vtreat on a small example\n",
    "In this exercise you will use vtreat to one-hot-encode a categorical variable on a small example. vtreat creates a treatment plan to transform categorical variables into indicator variables (coded \"lev\"), and to clean bad values out of numerical variables (coded \"clean\").\n",
    "\n",
    "To design a treatment plan use the function designTreatmentsZ()\n",
    "\n",
    "```\n",
    "treatplan <- designTreatmentsZ(data, varlist)\n",
    "```\n",
    "\n",
    "- data: the original training data frame\n",
    "- varlist: a vector of input variables to be treated (as strings).\n",
    "\n",
    "designTreatmentsZ() returns a list with an element scoreFrame: a data frame that includes the names and types of the new variables:\n",
    "\n",
    "```\n",
    "scoreFrame <- treatplan %>% \n",
    "            magrittr::use_series(scoreFrame) %>% \n",
    "            select(varName, origName, code)\n",
    "```\n",
    "\n",
    "- varName: the name of the new treated variable\n",
    "- origName: the name of the original variable that the treated variable comes from\n",
    "- code: the type of the new variable.\n",
    "    - \"clean\": a numerical variable with no NAs or NaNs\n",
    "    - \"lev\": an indicator variable for a specific level of the original categorical variable.\n",
    "    \n",
    "(magrittr::use_series() is an alias for $ that you can use in pipes.)\n",
    "\n",
    "For these exercises, we want varName where code is either \"clean\" or \"lev\":\n",
    "\n",
    "```\n",
    "newvarlist <- scoreFrame %>% \n",
    "             filter(code %in% c(\"clean\", \"lev\") %>%\n",
    "             magrittr::use_series(varName)\n",
    "```\n",
    "\n",
    "To transform the data set into all numerical and one-hot-encoded variables, use prepare():\n",
    "\n",
    "```\n",
    "data.treat <- prepare(treatplan, data, varRestrictions = newvarlist)\n",
    "```\n",
    "- treatplan: the treatment plan\n",
    "- data: the data frame to be treated\n",
    "- varRestrictions: the variables desired in the treated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dframe is in the workspace\n",
    "dframe\n",
    "#   color size popularity\n",
    "# 1      b   13  1.0785088\n",
    "# 2      r   11  1.3956245\n",
    "# 3      r   15  0.9217988\n",
    "# 4      r   14  1.2025453\n",
    "# 5      r   13  1.0838662\n",
    "# 6      b   11  0.8043527\n",
    "# 7      r    9  1.1035440\n",
    "# 8      g   12  0.8746332\n",
    "# 9      b    7  0.6947058\n",
    "# 10     b   12  0.8832502\n",
    "\n",
    "# Create and print a vector of variable names\n",
    "(vars <- c(\"color\", \"size\"))\n",
    "# [1] \"color\" \"size\"\n",
    "\n",
    "# Load the package vtreat\n",
    "library(vtreat)\n",
    "\n",
    "# Create the treatment plan\n",
    "treatplan <- designTreatmentsZ(dframe, vars)\n",
    "# [1] \"vtreat 1.2.0 inspecting inputs Sat Feb  8 09:34:11 2020\"\n",
    "# [1] \"designing treatments Sat Feb  8 09:34:11 2020\"\n",
    "# [1] \" have initial level statistics Sat Feb  8 09:34:11 2020\"\n",
    "# [1] \"design var color Sat Feb  8 09:34:11 2020\"\n",
    "# [1] \"design var size Sat Feb  8 09:34:11 2020\"\n",
    "# [1] \" scoring treatments Sat Feb  8 09:34:11 2020\"\n",
    "# [1] \"have treatment plan Sat Feb  8 09:34:11 2020\"\n",
    "\n",
    "# Examine the scoreFrame\n",
    "(scoreFrame <- treatplan %>%\n",
    "    use_series(scoreFrame) %>%\n",
    "    select(varName, origName, code))\n",
    "#         varName origName  code\n",
    "# 1    color_catP    color  catP\n",
    "# 2    size_clean     size clean\n",
    "# 3 color_lev_x_b    color   lev\n",
    "# 4 color_lev_x_g    color   lev\n",
    "# 5 color_lev_x_r    color   lev\n",
    "# We only want the rows with codes \"clean\" or \"lev\"\n",
    "(newvars <- scoreFrame %>%\n",
    "    filter(code %in% c(\"clean\", \"lev\")) %>%\n",
    "    use_series(varName))\n",
    "# [1] \"size_clean\"    \"color_lev_x_b\" \"color_lev_x_g\" \"color_lev_x_r\"\n",
    "\n",
    "# Create the treated training data\n",
    "(dframe.treat <- prepare(treatplan, dframe, varRestriction = newvars))\n",
    "#   size_clean color_lev_x_b color_lev_x_g color_lev_x_r\n",
    "# 1          13             1             0             0\n",
    "# 2          11             0             0             1\n",
    "# 3          15             0             0             1\n",
    "# 4          14             0             0             1\n",
    "# 5          13             0             0             1\n",
    "# 6          11             1             0             0\n",
    "# 7           9             0             0             1\n",
    "# 8          12             0             1             0\n",
    "# 9           7             1             0             0\n",
    "# 10         12             1             0             0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Novel levels\n",
    "When a level of a categorical variable is rare, sometimes it will fail to show up in training data. If that rare level then appears in future data, downstream models may not know what to do with it. When such novel levels appear, using model.matrix or caret::dummyVars to one-hot-encode will not work correctly.\n",
    "\n",
    "vtreat is a \"safer\" alternative to model.matrix for one-hot-encoding, because it can manage novel levels safely. vtreat also manages missing values in the data (both categorical and continuous).\n",
    "\n",
    "In this exercise you will see how vtreat handles categorical values that did not appear in the training set. The treatment plan treatplan and the set of variables newvars from the previous exercise are still in your workspace. dframe and a new data frame testframe are also in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print testframe\n",
    "testframe\n",
    "#   color size popularity\n",
    "# 1      g    7  0.9733920\n",
    "# 2      g    8  0.9122529\n",
    "# 3      y   10  1.4217153\n",
    "# 4      g   12  1.1905828\n",
    "# 5      g    6  0.9866464\n",
    "# 6      y    8  1.3697515\n",
    "# 7      b   12  1.0959387\n",
    "# 8      g   12  0.9161547\n",
    "# 9      g   12  1.0000460\n",
    "# 10     r    8  1.3137360\n",
    "\n",
    "# Use prepare() to one-hot-encode testframe\n",
    "(testframe.treat <- prepare(treatplan, testframe, varRestriction = newvars))\n",
    "#   size_clean color_lev_x_b color_lev_x_g color_lev_x_r\n",
    "# 1           7             0             1             0\n",
    "# 2           8             0             1             0\n",
    "# 3          10             0             0             0\n",
    "# 4          12             0             1             0\n",
    "# 5           6             0             1             0\n",
    "# 6           8             0             0             0\n",
    "# 7          12             1             0             0\n",
    "# 8          12             0             1             0\n",
    "# 9          12             0             1             0\n",
    "# 10          8             0             0             1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vtreat the bike rental data\n",
    "In this exercise you will create one-hot-encoded data frames of the July/August bike data, for use with xgboost later on.\n",
    "\n",
    "The data frames bikesJuly and bikesAugust are in the workspace.\n",
    "\n",
    "For your convenience, we have defined the variable vars with the list of variable columns for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The outcome column\n",
    "(outcome <- \"cnt\")\n",
    "\n",
    "# The input columns\n",
    "(vars <- c(\"hr\", \"holiday\", \"workingday\", \"weathersit\", \"temp\", \"atemp\", \"hum\", \"windspeed\"))\n",
    "\n",
    "# Load the package vtreat\n",
    "library(vtreat)\n",
    "\n",
    "# Create the treatment plan from bikesJuly (the training data)\n",
    "treatplan <- designTreatmentsZ(bikesJuly, vars, verbose = FALSE)\n",
    "\n",
    "# Get the \"clean\" and \"lev\" variables from the scoreFrame\n",
    "(newvars <- treatplan %>%\n",
    "  use_series(scoreFrame) %>%        \n",
    "  filter(code %in% c(\"clean\", \"lev\")) %>%  # get the rows you care about\n",
    "  use_series(varName))           # get the varName column\n",
    "\n",
    "# Prepare the training data\n",
    "bikesJuly.treat <- prepare(treatplan, bikesJuly,  varRestriction = newvars)\n",
    "\n",
    "# Prepare the test data\n",
    "bikesAugust.treat <- prepare(treatplan, bikesAugust,  varRestriction = newvars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the right number of trees for a gradient boosting machine\n",
    "In this exercise you will get ready to build a gradient boosting model to predict the number of bikes rented in an hour as a function of the weather and the type and time of day. You will train the model on data from the month of July.\n",
    "\n",
    "The July data is loaded into your workspace. Remember that bikesJuly.treat no longer has the outcome column, so you must get it from the untreated data: bikesJuly$cnt.\n",
    "\n",
    "You will use the xgboost package to fit the random forest model. The function xgb.cv() uses cross-validation to estimate the out-of-sample learning error as each new tree is added to the model. The appropriate number of trees to use in the final model is the number that minimizes the holdout RMSE.\n",
    "\n",
    "For this exercise, the key arguments to the xgb.cv() call are:\n",
    "\n",
    "- data: a numeric matrix.\n",
    "- label: vector of outcomes (also numeric).\n",
    "- nrounds: the maximum number of rounds (trees to build).\n",
    "- nfold: the number of folds for the cross-validation. 5 is a good number.\n",
    "- objective: \"reg:linear\" for continuous outcomes.\n",
    "- eta: the learning rate.\n",
    "- max_depth: depth of trees.\n",
    "- early_stopping_rounds: after this many rounds without improvement, stop.\n",
    "- verbose: 0 to stay silent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit an xgboost bike rental model and predict\n",
    "In this exercise you will fit a gradient boosting model using xgboost() to predict the number of bikes rented in an hour as a function of the weather and the type and time of day. You will train the model on data from the month of July and predict on data for the month of August.\n",
    "\n",
    "The datasets for July and August are loaded into your workspace. Remember the vtreat-ed data no longer has the outcome column, so you must get it from the original data (the cnt column).\n",
    "\n",
    "For convenience, the number of trees to use, ntrees from the previous exercise is in the workspace.\n",
    "\n",
    "The arguments to xgboost() are similar to those of xgb.cv()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the workspace\n",
    "ls()\n",
    "\n",
    "# The number of trees to use, as determined by xgb.cv\n",
    "ntrees\n",
    "\n",
    "# Run xgboost\n",
    "bike_model_xgb <- xgboost(data = as.matrix(bikesJuly.treat), # training data as matrix\n",
    "                   label = bikesJuly$cnt,  # column of outcomes\n",
    "                   nrounds = ntrees,       # number of trees to build\n",
    "                   objective = \"reg:linear\", # objective\n",
    "                   eta = 0.3,\n",
    "                   depth = 6,\n",
    "                   verbose = 0  # silent\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "bikesAugust$pred <- predict(bike_model_xgb, as.matrix(bikesAugust.treat))\n",
    "\n",
    "# Plot predictions (on x axis) vs actual bike rental count\n",
    "ggplot(bikesAugust, aes(x = pred, y = cnt)) + \n",
    "  geom_point() + \n",
    "  geom_abline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fit_an_xgboost_bike_rental_model_and_predict](./figures/fit_an_xgboost_bike_rental_model_and_predict.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the xgboost bike rental model\n",
    "In this exercise you will evaluate the gradient boosting model bike_model_xgb that you fit in the last exercise, using data from the month of August. You'll compare this model's RMSE for August to the RMSE of previous models that you've built.\n",
    "\n",
    "The dataset bikesAugust is in the workspace. You have already made predictions using the xgboost model; they are in the column pred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bikesAugust is in the workspace\n",
    "str(bikesAugust)\n",
    "\n",
    "# Calculate RMSE\n",
    "bikesAugust %>%\n",
    "  mutate(residuals = cnt - pred) %>%\n",
    "  summarize(rmse = sqrt(mean(residuals^2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though this gradient boosting made some negative predictions, overall it makes smaller errors than the previous two models. Perhaps rounding negative predictions up to zero is a reasonable tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the xgboost bike rental model\n",
    "You've now seen three different ways to model the bike rental data. For this example, you've seen that the gradient boosting model had the smallest RMSE. To finish up the course, let's compare the gradient boosting model's predictions to the other two models as a function of time.\n",
    "\n",
    "On completing this exercise, you will have completed the course. Congratulations! Now you have the tools to apply a variety of approaches to your regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print quasipoisson_plot\n",
    "print(quasipoisson_plot)\n",
    "\n",
    "# Print randomforest_plot\n",
    "print(randomforest_plot)\n",
    "\n",
    "# Plot predictions and actual bike rentals as a function of time (days)\n",
    "bikesAugust %>% \n",
    "  mutate(instant = (instant - min(instant))/24) %>%  # set start to 0, convert unit to days\n",
    "  gather(key = valuetype, value = value, cnt, pred) %>%\n",
    "  filter(instant < 14) %>% # first two weeks\n",
    "  ggplot(aes(x = instant, y = value, color = valuetype, linetype = valuetype)) + \n",
    "  geom_point() + \n",
    "  geom_line() + \n",
    "  scale_x_continuous(\"Day\", breaks = 0:14, labels = 0:14) + \n",
    "  scale_color_brewer(palette = \"Dark2\") + \n",
    "  ggtitle(\"Predicted August bike rentals, Gradient Boosting model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient boosting pattern captures rental variations due to time of day and other factors better than the previous models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visualize_the_xgboost_bike_rental_model_1](./figures/visualize_the_xgboost_bike_rental_model_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visualize_the_xgboost_bike_rental_model_3](./figures/visualize_the_xgboost_bike_rental_model_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visualize_the_xgboost_bike_rental_model_2](./figures/visualize_the_xgboost_bike_rental_model_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jupyterlab]",
   "language": "python",
   "name": "conda-env-jupyterlab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
